# My Project working with Sewell BMW

### Context ###
In late 2021, the Sewell Automotive group acquired Classic BMW of Plano. Sewell, with a high standard of excellence and success, strives for the best, and this new dealership served as a great way to witness this drive. Given the circumstances of the dealership, like the location, demographic, and size, the bar was high. We should be among the top 5 BMW dealers in the nation and earn the corporate BMW Center of Excellence award.

### Problem ###
To get there, certain changes had to be made. One area identified included increasing the percentage of loyal customers, which seemed unusually low given the dealer's circumstances. It turns out that the existing method of identifying loyal customers would overlook customers with minor changes to their information, like a different last name, different address, or different vehicle. Traditional alogrithms look for exact matches, leaving too much on the table in this case.1 For several months, sales managers would manually scrape spreadsheets with thousands of rows looking to identify potential missed matches. An inefficient and redundant process for a rather simple task that wastes valuable time.

### Solution ###
Artificial intelligence. A major benefit of AI includes its ability to draw human-like conclusions based on given information rather than follow a strict set of rules. I initially tried using SpaCy for a semantic-based similarity score that uses natural language processing, but for rows of customer information, it does not serve well. A general large language model would best satisfy the goal of this project. Using an OpenAI API key and the GPT-4o-mini model, the program feeds the LLM the sheets of information in a prompt. The pandas library made extracting and working with the data from excel files quite manageable. The prompt, role, and variability of the model can, of course, be modified to adjust the purpose of the program; in this case, some of them are hard-coded to provide consistent results. The GPT model outputs a table with the information of interest. To make this program user-friendly, Streamlit proved useful in making a simple, locally-hosted interface. Results can be downloaded to a file, ready to go for the user.

### Challenges ###
Something tricky about generative LLMs remains their inconsistency in output. The model can miss some rows as it is not perfectly optimized for data analysis to this level of detail. To mitigate this, a low temperature (variability) and specific role and prompt made results more reliable. Additionally, in cases of inconsistent results, the program runs the data through the model multiple times to compare outputs, helping reduce one-off unusual outputs from reaching the user. Naturally, this increases the usage of the API, but at just a fraction of a cent per usage, the cost increase is virtually negligible. However, there still exist cases where model rarely identifies rows it should, leaving an evident stain on the usefulness of this model as this is the issue it is designed to redeem. This must improve, and I am seeking solutions.
Further, OpenAI's token limit can also prove limiting in a project deployed at this scale. With a maximum context window of 128000 tokens on GPT-4o-mini model, this program is good for files with up to about 2000 rows. This may cause issues as the usage expands to larger time-frames and levels, at which point a solution must be worked.
This program certainly does not perfectly capture all missing loyal sales, but it must boost the existing loyal percentage to a certain threshold. If reached, then the purpose of the program has been fulfilled.



#### 1 ####
There are three sheets. One with all sales of the last 5 years (max. time frame for a customer to be considered "loyal"), a second with the counted loyal sales for that month, and a third with the actual loyal sales of the month (some of which did not get counted for the reasons listed). The program must find customers who are actually loyal (appear in the third sheet AND the first one) but were not counted by the existing algorithm (not present in the second sheet). 